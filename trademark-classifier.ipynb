{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6f843127e255497ba8ac6752d5caa2af":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_3f29664ab604404386304b06371a63c3","IPY_MODEL_ddd727c8995c41cf94114826160d68cc"],"layout":"IPY_MODEL_b97eb46852d745e3980d01e7fc200df0"}},"3f29664ab604404386304b06371a63c3":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c912358900745ceaeb9b3743938f504","placeholder":"​","style":"IPY_MODEL_0d67f73dbe1d4a509664f14883b0e063","value":"0.012 MB of 0.012 MB uploaded\r"}},"ddd727c8995c41cf94114826160d68cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd261a1feeba419d8cd80da9b2bbd156","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_359c8ce5ce2f4bf7ba791459866b8246","value":1}},"b97eb46852d745e3980d01e7fc200df0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c912358900745ceaeb9b3743938f504":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d67f73dbe1d4a509664f14883b0e063":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd261a1feeba419d8cd80da9b2bbd156":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"359c8ce5ce2f4bf7ba791459866b8246":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"427a3919ef9a407b8235201c590a3621":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_d935a2662c1c4b69be1abf0e652ff4ad","IPY_MODEL_863386721a954f78b8d5a2b91234013c"],"layout":"IPY_MODEL_0864db1af6054882b129d8ab49a7f453"}},"d935a2662c1c4b69be1abf0e652ff4ad":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_989e49b77f8b4416905a62d7ee853411","placeholder":"​","style":"IPY_MODEL_ef5475a1f9f242f2a842893aa141825f","value":"Waiting for wandb.init()...\r"}},"863386721a954f78b8d5a2b91234013c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a2da1cf34f646008769deaf7a60914b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_66a02654ba4048a9bab0f4b3f793023d","value":1}},"0864db1af6054882b129d8ab49a7f453":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"989e49b77f8b4416905a62d7ee853411":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef5475a1f9f242f2a842893aa141825f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a2da1cf34f646008769deaf7a60914b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66a02654ba4048a9bab0f4b3f793023d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9166364,"sourceType":"datasetVersion","datasetId":5538626}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Assuming data is loaded in a DataFrame called df\ndf = pd.read_json(\"/kaggle/input/trademark-dataset/idmanual.json\")  # Replace with the actual data loading method\n\n# Only use rows with 'status' == 'A'\ndf = df[df['status'] == 'A']\n\n# Preprocess text and labels\nX = df['description'].values\ny = df['class_id'].values\n\n# Convert class labels to numeric\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ny = label_encoder.fit_transform(y)\n\n# Split data into training and validation sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"id":"B5ctPeEYhTRW","execution":{"iopub.status.busy":"2024-08-14T08:24:00.470613Z","iopub.execute_input":"2024-08-14T08:24:00.471384Z","iopub.status.idle":"2024-08-14T08:24:01.922265Z","shell.execute_reply.started":"2024-08-14T08:24:00.471351Z","shell.execute_reply":"2024-08-14T08:24:01.921124Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, AdamW\n\nclass TrademarkDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmax_len = 128\n\n# Create datasets\ntrain_dataset = TrademarkDataset(X_train, y_train, tokenizer, max_len)\nval_dataset = TrademarkDataset(X_val, y_val, tokenizer, max_len)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n\n# Define the model\nclass TrademarkClassifier(nn.Module):\n    def __init__(self, n_classes):\n        super(TrademarkClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.drop = nn.Dropout(p=0.3)\n        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        pooled_output = outputs.pooler_output\n        output = self.drop(pooled_output)\n        return self.out(output)\n\n# Initialize the model\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = TrademarkClassifier(len(label_encoder.classes_))\nmodel = model.to(device)\n\n# Define optimizer and loss function\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\nloss_fn = nn.CrossEntropyLoss().to('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"uhpZTHXjk8Aa","execution":{"iopub.status.busy":"2024-08-14T08:24:03.748048Z","iopub.execute_input":"2024-08-14T08:24:03.749010Z","iopub.status.idle":"2024-08-14T08:24:13.770445Z","shell.execute_reply.started":"2024-08-14T08:24:03.748973Z","shell.execute_reply":"2024-08-14T08:24:13.769479Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7adae7ead24a431695f849aabeab8261"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07b7d439ac92409c88cced88443fc465"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db8d789909b3477f97bf7f05c595df7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94a7d06a35db44daa7f379b60ee0546f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c808ee1206b4752b4217891e36893d7"}},"metadata":{}}]},{"cell_type":"code","source":"import wandb\n\n# Initialize WandB\nwandb.login()\nwandb.init(project=\"trademark-classification\", settings=wandb.Settings(start_method=\"fork\"))\nwandb.watch(model, log=\"all\")\n\n# Training and evaluation functions\ndef train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n    model = model.train()\n    losses = []\n    correct_predictions = 0\n\n    for d in data_loader:\n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        labels = d[\"label\"].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        _, preds = torch.max(outputs, dim=1)\n        loss = loss_fn(outputs, labels)\n\n        correct_predictions += torch.sum(preds == labels)\n        losses.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    return correct_predictions.double() / n_examples, np.mean(losses)\n\ndef eval_model(model, data_loader, loss_fn, device, n_examples):\n    model = model.eval()\n    losses = []\n    correct_predictions = 0\n\n    with torch.no_grad():\n        for d in data_loader:\n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            labels = d[\"label\"].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            _, preds = torch.max(outputs, dim=1)\n            loss = loss_fn(outputs, labels)\n\n            correct_predictions += torch.sum(preds == labels)\n            losses.append(loss.item())\n\n    return correct_predictions.double() / n_examples, np.mean(losses)\n\n#Training loop\nnum_epochs = 5\nbest_accuracy = 0\n\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch + 1}/{num_epochs}')\n    print('-' * 10)\n\n    train_acc, train_loss = train_epoch(\n        model,\n        train_loader,\n        loss_fn,\n        optimizer,\n        device,\n        None,\n        len(X_train)\n    )\n\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n\n    val_acc, val_loss = eval_model(\n        model,\n        val_loader,\n        loss_fn,\n        device,\n        len(X_val)\n    )\n\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n\n    wandb.log({\n        \"train_loss\": train_loss,\n        \"train_acc\": train_acc,\n        \"val_loss\": val_loss,\n        \"val_acc\": val_acc\n    })\n\n    if val_acc > best_accuracy:\n        torch.save({'model_state_dict': model.state_dict(),\n                   'label_encoder': label_encoder}, 'trademark_classifier.pth')\n        best_accuracy = val_acc\n\nwandb.finish()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":393,"referenced_widgets":["6f843127e255497ba8ac6752d5caa2af","3f29664ab604404386304b06371a63c3","ddd727c8995c41cf94114826160d68cc","b97eb46852d745e3980d01e7fc200df0","2c912358900745ceaeb9b3743938f504","0d67f73dbe1d4a509664f14883b0e063","fd261a1feeba419d8cd80da9b2bbd156","359c8ce5ce2f4bf7ba791459866b8246","427a3919ef9a407b8235201c590a3621","d935a2662c1c4b69be1abf0e652ff4ad","863386721a954f78b8d5a2b91234013c","0864db1af6054882b129d8ab49a7f453","989e49b77f8b4416905a62d7ee853411","ef5475a1f9f242f2a842893aa141825f","8a2da1cf34f646008769deaf7a60914b","66a02654ba4048a9bab0f4b3f793023d"]},"id":"yknVmlS0k9os","outputId":"84dd6716-9bdd-4fc4-a81b-e60ba3b47c59","execution":{"iopub.status.busy":"2024-08-14T08:24:15.494958Z","iopub.execute_input":"2024-08-14T08:24:15.495542Z","iopub.status.idle":"2024-08-14T09:09:11.998914Z","shell.execute_reply.started":"2024-08-14T08:24:15.495511Z","shell.execute_reply":"2024-08-14T09:09:11.997984Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahulradhesh\u001b[0m (\u001b[33mrahulradhesh-vellore-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240814_082448-dip2balp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rahulradhesh-vellore-institute-of-technology/trademark-classification/runs/dip2balp' target=\"_blank\">upbeat-plasma-14</a></strong> to <a href='https://wandb.ai/rahulradhesh-vellore-institute-of-technology/trademark-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rahulradhesh-vellore-institute-of-technology/trademark-classification' target=\"_blank\">https://wandb.ai/rahulradhesh-vellore-institute-of-technology/trademark-classification</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rahulradhesh-vellore-institute-of-technology/trademark-classification/runs/dip2balp' target=\"_blank\">https://wandb.ai/rahulradhesh-vellore-institute-of-technology/trademark-classification/runs/dip2balp</a>"},"metadata":{}},{"name":"stdout","text":"Epoch 1/5\n----------\nTrain loss 1.2826075333568872 accuracy 0.6807053889492434\nVal   loss 0.5547815478122484 accuracy 0.853071948261924\nEpoch 2/5\n----------\nTrain loss 0.40770890011106564 accuracy 0.891766251484298\nVal   loss 0.43709824150292764 accuracy 0.8822756669361358\nEpoch 3/5\n----------\nTrain loss 0.22314368232914525 accuracy 0.939011141709406\nVal   loss 0.4215591257251498 accuracy 0.8882376717865804\nEpoch 4/5\n----------\nTrain loss 0.13537663136885209 accuracy 0.9621030292311968\nVal   loss 0.40234047631313347 accuracy 0.8999595796281326\nEpoch 5/5\n----------\nTrain loss 0.0947614638765381 accuracy 0.9735984436977337\nVal   loss 0.42021978340752614 accuracy 0.9019805982215036\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>▁▆▇██</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr><tr><td>val_acc</td><td>▁▅▆██</td></tr><tr><td>val_loss</td><td>█▃▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_acc</td><td>0.9736</td></tr><tr><td>train_loss</td><td>0.09476</td></tr><tr><td>val_acc</td><td>0.90198</td></tr><tr><td>val_loss</td><td>0.42022</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">upbeat-plasma-14</strong> at: <a href='https://wandb.ai/rahulradhesh-vellore-institute-of-technology/trademark-classification/runs/dip2balp' target=\"_blank\">https://wandb.ai/rahulradhesh-vellore-institute-of-technology/trademark-classification/runs/dip2balp</a><br/> View project at: <a href='https://wandb.ai/rahulradhesh-vellore-institute-of-technology/trademark-classification' target=\"_blank\">https://wandb.ai/rahulradhesh-vellore-institute-of-technology/trademark-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240814_082448-dip2balp/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}]},{"cell_type":"code","source":"def predict(text, model, tokenizer, max_len):\n    encoding = tokenizer.encode_plus(\n        text,\n        add_special_tokens=True,\n        max_length=max_len,\n        return_token_type_ids=False,\n        padding='max_length',\n        truncation=True,\n        return_attention_mask=True,\n        return_tensors='pt',\n    )\n    input_ids = encoding['input_ids'].to(device)\n    attention_mask = encoding['attention_mask'].to(device)\n\n    model.eval()\n    with torch.no_grad():\n        output = model(input_ids, attention_mask)\n        _, prediction = torch.max(output, dim=1)\n\n    return label_encoder.inverse_transform(prediction.cpu().numpy())[0]\n\n# Test with samples input\nsample_text1 = \"Laptop carrying cases\"\npredicted_class1 = predict(sample_text, model, tokenizer, max_len)\nprint(f\"Predicted class for '{sample_text1}': {predicted_class1}\")\n\nsample_text2 = \"Chennai Super Kings\"\npredicted_class2 = predict(sample_text2, model, tokenizer, max_len)\nprint(f\"Predicted class for '{sample_text2}': {predicted_class2}\")\n\nsample_text3 = \"Ankle Fracture\"\npredicted_class3 = predict(sample_text3, model, tokenizer, max_len)\nprint(f\"Predicted class for '{sample_text3}': {predicted_class3}\")\n\nsample_text4 = \"Sports Wear\"\npredicted_class4 = predict(sample_text4, model, tokenizer, max_len)\nprint(f\"Predicted class for '{sample_text4}': {predicted_class4}\")\n\nsample_text5 = \"Shortage of Money\"\npredicted_class5 = predict(sample_text5, model, tokenizer, max_len)\nprint(f\"Predicted class for '{sample_text5}': {predicted_class5}\")","metadata":{"id":"jJf4tcZTlC_x","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b56fcf07-6312-4a84-ca49-e8116cea9703","execution":{"iopub.status.busy":"2024-08-14T09:52:06.162638Z","iopub.execute_input":"2024-08-14T09:52:06.163745Z","iopub.status.idle":"2024-08-14T09:52:06.244925Z","shell.execute_reply.started":"2024-08-14T09:52:06.163709Z","shell.execute_reply":"2024-08-14T09:52:06.243732Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Predicted class for 'Laptop carrying cases': 009\nPredicted class for 'Chennai Super Kings': 028\nPredicted class for 'Ankle Fracture': 010\nPredicted class for 'Sports Wear': 025\nPredicted class for 'Shortage of Money': 036\n","output_type":"stream"}]}]}